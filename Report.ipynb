{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'Cohort Analysis for Ironhack Payments'\n",
    "format: \n",
    "    html: \n",
    "        toc: true\n",
    "        page-layout: full\n",
    "        grid:\n",
    "            body-width: 1600px\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid confusion, we will disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First, let's make sure we have the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the rows bellow to install the modules\n",
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install seaborn\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "When using environments, replace !pip with %pip\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now let's import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> And now let's load the CSV files we'll be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_df = pd.read_csv('project_dataset/extract - cash request - data analyst.csv')\n",
    "fees_df = pd.read_csv('project_dataset/extract - fees - data analyst - .csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now we can start our exploratory analysis.\n",
    "\n",
    ":::{.callout-note}\n",
    "With `pandas` we can use [`.head()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) or [`.tail()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html), [`.info()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html), [`.shape`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html) and [`.describe()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) to get a quick view of the data.  \n",
    "We can also use [`.sample(<number of samples>)`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html) to see some randomly selected rows.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fees_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fees_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fees_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We already notice that we have some columns with **numbers** (`amount` in `cash_df` and `total_amount` in `fees_df`), **datetime** (`created_at`, `updated_at` etc) and **object** (`status`, `transfer_type` etc).  \n",
    "> Let's list the `unique()` values for latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"status: {cash_df['status'].unique()}\")\n",
    "print(f\"transfer_type: {cash_df['transfer_type'].unique()}\")\n",
    "print(f\"recover_status: {cash_df['recovery_status'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "Let's make a note that we have some `nan` (not-a-number) values - we'll get to that in a bit.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"type: {fees_df['type'].unique()}\")\n",
    "print(f\"status: {fees_df['status'].unique()}\")\n",
    "print(f\"category: {fees_df['category'].unique()}\")\n",
    "print(f\"charge_moment: {fees_df['charge_moment'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Analysis Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now that we've got an idea of the data, let's check if there are any missing (empty or `null`) values in our dataframes.  \n",
    "> We can use [`.isna()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isna.html) (or its alias - `isnull()`) and the inverse [`notna()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.notna.html#pandas.DataFrame.notna) to check if a value is **null**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_df.isna().sum() # by appending .sum() we get a sum of Null values, per row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We notice that the `user_id` column has 2103 `nan` values - but we also have a `deleted_account_id` column which contains id's for deleted user accounts.  \n",
    "Let's check if there are rows where both `user_id` and `deleted_account_id` are empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cash_df['user_id'].isna() & (cash_df['deleted_account_id'].isna())).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For the rest of the columns `nan` values are not necessarily a problem, so we won't investigate them further.  \n",
    "> Now let's look at `fees_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fees_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We notice there are 4 rows that don't have a value for `cash_request_id`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "The `isna()` function creates a **mask** with `True/False` where the values in the dataframe are `Null`. We can then use this **mask** to select rows from the dataframe\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fees_df[fees_df['cash_request_id'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As the `cash_request_id` is a reference to the `id` from the `cash_df` dataframe, and there are only 4 rows, we can safely remove/ignore these instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "We can use the inverse method `.notna()` and re-assign `fees_df` to the dataframe without `Null` values.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fees_df = fees_df[fees_df['cash_request_id'].notna()]\n",
    "fees_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Before we can actually do our analysis, we need to prepare the dataframes for easier manipulation.  \n",
    "> For this we'll convert the datetime values to the correct type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_columns_cash = ['created_at', 'updated_at', 'moderated_at', 'reimbursement_date',\\\n",
    "                          'cash_request_received_date', 'money_back_date', 'send_at',\\\n",
    "                         'reco_creation', 'reco_last_update']\n",
    "\n",
    "for column in datetime_columns_cash:\n",
    "    cash_df[column] = pd.to_datetime(cash_df[column]) # in python3.11.8 we need to add format='mixed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_columns_fees = ['created_at', 'updated_at', 'paid_at', 'from_date', 'to_date']\n",
    "for column in datetime_columns_fees:\n",
    "    fees_df[column] = pd.to_datetime(fees_df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fees_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now that our dataframes are cleaned and the values are of the correct type, we can start our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Frequency of Service Usage\n",
    "\n",
    "**Understand how often users from each cohort utilize IronHack Payments' cash advance services over time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To determine the utilisation of the IronHack Payments' service, we first need to group users in cohorts.  \n",
    "> We will use the `created_at` field to generate the user cohorts, split by months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_df['cohort'] = cash_df['created_at'].dt.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_counts = cash_df.groupby('cohort').size()\n",
    "cohort_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By analysing the cohort sizes, we can deduct that the data is incomplete for the first (2019-11) and last (2020-11) months - to avoid confusion we will hide these months from our plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_counts = cohort_counts.iloc[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "freq_use = sns.barplot(x=cohort_counts.index.astype(str), y=cohort_counts.values, palette='magma_r')\n",
    "plt.title('Number of Cash Requests per Cohort')\n",
    "plt.xlabel('Cohort')\n",
    "plt.ylabel('Number of Cash Requests')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "# we can also show the values directly in the plot\n",
    "for i in freq_use.containers:\n",
    "    freq_use.bar_label(i,)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Incident Rate\n",
    "**Determine the incident rate, specifically focusing on payment incidents, for each cohort. Identify if there are variations in incident rates among different cohorts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To determine the **Incident Rate** we will look at the `fees_df` dataframe.  \n",
    "> More specifically, we will consider an **incident** all the rows where `category` is *rejected_direct_debit* or *month_delay_on_payment*.  \n",
    "> To actually determine the **rate** we need to calculate the percentage of *incidents* from all the fees payments.\n",
    ">  \n",
    "> First we will merge the two dataframes using the `id` from `cash_df` and `cash_request_id` from `fees_df` as indexes.\n",
    ">  \n",
    "> We will do a `inner` merge to assure that only the rows where there is a match between `id` and `cash_request_id` in both dataframes are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(cash_df, fees_df, left_on='id', right_on='cash_request_id', how='inner', suffixes=['_cash', '_fees'])\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can now create a new dataframe with only the relevant columns: `user_id`, `type`, `category` and `cohort`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incidents_df = merged_df[['user_id', 'type', 'category', 'cohort']]\n",
    "incidents_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We calculate the incident rate by:  \n",
    "> - getting the count of all payments  \n",
    "> - counting all the incidents - all the payments where `category` is not null.  \n",
    ">  \n",
    "> We group them by `cohort` to create a series so we can see the distribution per cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_payments = len(incidents_df)\n",
    "incidents = incidents_df[incidents_df['category'].notna()].groupby('cohort').size()\n",
    "incident_rate = (incidents / total_payments) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As `incidents` now contains a series, we need to convert it to a dataframe to plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incident_rate_df = incident_rate.to_frame(name='Incident Rate per Cohort').reset_index()\n",
    "incident_rate_df.columns = ['Cohort', 'Incident Rate (%)']\n",
    "display(incident_rate_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "We will also export this dataframe to a csv file as we'll need it later.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(incident_rate_df, 'incidents.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "incident_plot = sns.barplot(data=incident_rate_df, x='Cohort', y='Incident Rate (%)', palette='magma_r')\n",
    "plt.title('Incident Rate per Cohort')\n",
    "plt.ylabel('Incident Rate (%)')\n",
    "plt.xlabel('Cohort')\n",
    "incident_plot.bar_label(incident_plot.containers[0], fmt='%.2f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Revenue Generated by the Cohort\n",
    "**Calculate the total revenue generated by each cohort over months to assess the financial impact of user behavior.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To calculate the revenue generate per cohort we need to again look at both `cash_df` and `fees_df` dataframes.  \n",
    "> We will use `merge_df` with the columns `user_id` and `cohort` from `cash_df` and `total_amount` from `fees_df`.  \n",
    "> We'll also take the `paid_at` column from `fees_df` as we'll consider revenue only payments that have been actually paid - we'll ignore rows that don't have a value in the `paid_at` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_df = merged_df[['user_id', 'cohort', 'total_amount', 'paid_at']]\n",
    "revenue_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now we can filter out the rows that don't have a value for `paid_at`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_df = revenue_df[revenue_df['paid_at'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> And now we can calculate the revenue generated by each cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_revenue = revenue_df.groupby('cohort')['total_amount'].sum()\n",
    "cohort_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save a csv for later use\n",
    "pd.DataFrame.to_csv(revenue_df, 'revenue.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "revenue_plot = sns.barplot(x=cohort_revenue.index.astype(str), y=cohort_revenue.values, palette='magma_r')\n",
    "plt.title('Revenue Generated per Cohort')\n",
    "plt.xlabel('Cohort')\n",
    "plt.ylabel('Revenue')\n",
    "# plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "revenue_plot.bar_label(revenue_plot.containers[0], fmt='{:,.0f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. New Relevant Metric \n",
    "**Propose and calculate a new relevant metric that provides additional insights into user behavior or the performance of IronHack Payments' services.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Given the data we have there are a couple of relevant metrics we could investigate that would provide additional insights into user behaviour.  \n",
    "> One such metric could be **churn rate** - the percentage of users who stop using the IronHack Payments.  \n",
    ">\n",
    "> To calculate the **churn_rate** we will use the `cash_df` dataframe and calculate the percentage of deleted user accounts per cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleted_accounts = cash_df.groupby('cohort')['deleted_account_id'].count()\n",
    "deleted_df = deleted_accounts\n",
    "deleted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we already have a dataframe with created account per cohort\n",
    "churn_df = pd.merge(left=deleted_df, right=cohort_counts.reset_index(), left_on='cohort', right_on='cohort')\n",
    "churn_df.columns = [['cohort', 'deleted_accounts', 'new_accounts']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "To calculate the **churn rate** we use the formula:  \n",
    "$$\n",
    "{Churn Rate} = \\frac{Number of Deleted Accounts}{Number of New Accounts + Number of Deleted Accounts} \\times 100\n",
    "$$\n",
    ":::\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a function that calculates the churn rate for a row\n",
    "def calculate_churn_rate(row):\n",
    "    if row['new_accounts'] == 0:  # To avoid division by zero\n",
    "        return 0\n",
    "    else:\n",
    "        return float(row['deleted_accounts'] / (row['new_accounts'] + row['deleted_accounts']) * 100)\n",
    "\n",
    "# and we call `.apply` on the `churn_df`; this will return a series\n",
    "churn_rates = churn_df.apply(calculate_churn_rate, axis=1)\n",
    "\n",
    "# and we assign the series to a new column\n",
    "churn_df['churn_rate'] = churn_rates\n",
    "\n",
    "churn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "churn_plot = sns.barplot(x=churn_df['cohort'].unstack(0), y=churn_rates.values, data=churn_df, palette='magma_r')\n",
    "plt.title('Churn Rate per Cohort')\n",
    "plt.xlabel('Cohort')\n",
    "plt.ylabel('Churn Rate (%)')\n",
    "# plt.xticks(rotation=45)\n",
    "churn_plot.bar_label(churn_plot.containers[0], fmt='%.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final considerations\n",
    "\n",
    "The exported CSV files will be used for importing in Tableau to generate a dashboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
